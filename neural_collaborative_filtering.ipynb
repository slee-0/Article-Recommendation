{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommendation System Using Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data = pd.read_csv('data/interactions_small.csv', index_col=0)\n",
    "book_data = pd.read_csv('data/book_metadata.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>is_read</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text_incomplete</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>1384</td>\n",
       "      <td>1bad0122cebb4aa9213f9fe1aa281f66</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wed May 09 09:33:44 -0700 2007</td>\n",
       "      <td>Wed May 09 09:33:44 -0700 2007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>1376</td>\n",
       "      <td>eb6e502d0c04d57b43a5a02c21b64ab4</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wed May 09 09:33:18 -0700 2007</td>\n",
       "      <td>Wed May 09 09:33:18 -0700 2007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>30119</td>\n",
       "      <td>787564bef16cb1f43e0f641ab59d25b7</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Jan 13 13:44:20 -0800 2007</td>\n",
       "      <td>Wed Mar 22 11:45:08 -0700 2017</td>\n",
       "      <td>Tue Mar 01 00:00:00 -0800 1983</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72fb0d0087d28c832f15776b0d936598</td>\n",
       "      <td>24769928</td>\n",
       "      <td>8c80ee74743d4b3b123dd1a2e0c0bcac</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wed Apr 27 11:05:51 -0700 2016</td>\n",
       "      <td>Wed Apr 27 11:05:52 -0700 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72fb0d0087d28c832f15776b0d936598</td>\n",
       "      <td>30119</td>\n",
       "      <td>2a83589fb597309934ec9b1db5876aaf</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jun 04 18:58:08 -0700 2012</td>\n",
       "      <td>Mon Jun 04 18:58:13 -0700 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id   book_id  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d      1384   \n",
       "1  8842281e1d1347389f2ab93d60773d4d      1376   \n",
       "2  8842281e1d1347389f2ab93d60773d4d     30119   \n",
       "3  72fb0d0087d28c832f15776b0d936598  24769928   \n",
       "4  72fb0d0087d28c832f15776b0d936598     30119   \n",
       "\n",
       "                          review_id  is_read  rating review_text_incomplete  \\\n",
       "0  1bad0122cebb4aa9213f9fe1aa281f66     True       4                    NaN   \n",
       "1  eb6e502d0c04d57b43a5a02c21b64ab4     True       4                    NaN   \n",
       "2  787564bef16cb1f43e0f641ab59d25b7     True       5                    NaN   \n",
       "3  8c80ee74743d4b3b123dd1a2e0c0bcac    False       0                    NaN   \n",
       "4  2a83589fb597309934ec9b1db5876aaf     True       3                    NaN   \n",
       "\n",
       "                       date_added                    date_updated  \\\n",
       "0  Wed May 09 09:33:44 -0700 2007  Wed May 09 09:33:44 -0700 2007   \n",
       "1  Wed May 09 09:33:18 -0700 2007  Wed May 09 09:33:18 -0700 2007   \n",
       "2  Sat Jan 13 13:44:20 -0800 2007  Wed Mar 22 11:45:08 -0700 2017   \n",
       "3  Wed Apr 27 11:05:51 -0700 2016  Wed Apr 27 11:05:52 -0700 2016   \n",
       "4  Mon Jun 04 18:58:08 -0700 2012  Mon Jun 04 18:58:13 -0700 2012   \n",
       "\n",
       "                          read_at started_at  \n",
       "0                             NaN        NaN  \n",
       "1                             NaN        NaN  \n",
       "2  Tue Mar 01 00:00:00 -0800 1983        NaN  \n",
       "3                             NaN        NaN  \n",
       "4                             NaN        NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10241, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use only the data where is_read==True\n",
    "ratings = ratings_data[ratings_data['is_read']==True]\n",
    "ratings = ratings[[']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix where the rows are users and columns are books\n",
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('user_id')['rating'].count()\n",
    "    \n",
    "    unique_users = ratings.user_id.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.user_id.map(user_to_index)\n",
    "    \n",
    "    unique_books = ratings.book_id.unique()\n",
    "    book_to_index = {old: new for new, old in enumerate(unique_books)}\n",
    "    new_books = ratings.book_id.map(book_to_index)\n",
    "    \n",
    "    n_users = unique_users.shape[0]\n",
    "    n_books = unique_books.shape[0]\n",
    "    \n",
    "    X = pd.DataFrame({'user_id': new_users, 'book_id': new_books})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_books), (X, y), (user_to_index, book_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 1853 users, 4064 books\n",
      "Dataset shape: (10241, 2)\n",
      "Target shape: (10241,)\n"
     ]
    }
   ],
   "source": [
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} books')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsIterator:\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        \n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a dense network with embedding layers.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        n_users:            \n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_movies: \n",
    "            Number of unique movies in the dataset.\n",
    "\n",
    "        n_factors: \n",
    "            Number of columns in the embeddings matrix.\n",
    "\n",
    "        embedding_dropout: \n",
    "            Dropout rate to apply right after embeddings layer.\n",
    "\n",
    "        hidden:\n",
    "            A single integer or a list of integers defining the number of \n",
    "            units in hidden layer(s).\n",
    "\n",
    "        dropouts: \n",
    "            A single integer or a list of integers defining the dropout \n",
    "            layers rates applyied right after each of hidden layers.\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_movies,\n",
    "                 n_factors=50, embedding_dropout=0.02, \n",
    "                 hidden=10, dropouts=0.2):\n",
    "        super().__init__()\n",
    "        hidden = get_list(hidden)\n",
    "        dropouts = get_list(dropouts)\n",
    "        n_last = hidden[-1]\n",
    "        \n",
    "        def gen_layers(n_in):\n",
    "            \"\"\"\n",
    "            A generator that yields a sequence of hidden layers and \n",
    "            their activations/dropouts.\n",
    "            \n",
    "            Note that the function captures `hidden` and `dropouts` \n",
    "            values from the outer scope.\n",
    "            \"\"\"\n",
    "            nonlocal hidden, dropouts\n",
    "            assert len(dropouts) <= len(hidden)\n",
    "            \n",
    "            for n_out, rate in itertools.zip_longest(hidden, dropouts):\n",
    "                yield nn.Linear(n_in, n_out)\n",
    "                yield nn.ReLU()\n",
    "                if rate is not None and rate > 0.:\n",
    "                    yield nn.Dropout(rate)\n",
    "                n_in = n_out\n",
    "            \n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n",
    "        self.fc = nn.Linear(n_last, 1)\n",
    "        self._init()\n",
    "        \n",
    "    def forward(self, users, movies, minmax=None):\n",
    "        features = torch.cat([self.u(users), self.m(movies)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "    \n",
    "    def _init(self):\n",
    "        \"\"\"\n",
    "        Setup embeddings and hidden layers with reasonable initial values.\n",
    "        \"\"\"\n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "                \n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "    \n",
    "    \n",
    "def get_list(n):\n",
    "    if isinstance(n, (int, float)):\n",
    "        return [n]\n",
    "    elif hasattr(n, '__iter__'):\n",
    "        return list(n)\n",
    "    raise TypeError('layers configuraiton should be a single number or a list of numbers')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = ratings.rating.min(), ratings.rating.max()\n",
    "minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbeddingNet(\n",
    "    n_users=n, n_movies=m, \n",
    "    n_factors=150, hidden=[500, 500, 500], \n",
    "    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1853 4064\n"
     ]
    }
   ],
   "source": [
    "print(n,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] train: 2.2937 - val: 1.7459\n",
      "[002/030] train: 1.8434 - val: 1.6205\n",
      "[003/030] train: 1.6230 - val: 1.6450\n",
      "[004/030] train: 1.5912 - val: 1.5565\n",
      "[005/030] train: 1.4135 - val: 1.5030\n",
      "[006/030] train: 1.3128 - val: 1.4836\n",
      "[007/030] train: 1.1543 - val: 1.4513\n",
      "[008/030] train: 1.0231 - val: 1.4518\n",
      "[009/030] train: 0.9197 - val: 1.5015\n",
      "[010/030] train: 0.8417 - val: 1.5517\n",
      "[011/030] train: 0.7760 - val: 1.5771\n",
      "[012/030] train: 0.7391 - val: 1.6222\n",
      "[013/030] train: 0.7014 - val: 1.6508\n",
      "[014/030] train: 0.6607 - val: 1.6804\n",
      "[015/030] train: 0.6251 - val: 1.7082\n",
      "[016/030] train: 0.5864 - val: 1.7243\n",
      "[017/030] train: 0.5424 - val: 1.7874\n",
      "[018/030] train: 0.5063 - val: 1.8390\n",
      "[019/030] train: 0.4735 - val: 1.8597\n",
      "[020/030] train: 0.4415 - val: 1.8606\n",
      "[021/030] train: 0.4185 - val: 1.8435\n",
      "[022/030] train: 0.3842 - val: 1.9116\n",
      "[023/030] train: 0.3687 - val: 1.9735\n",
      "[024/030] train: 0.3348 - val: 1.9646\n",
      "[025/030] train: 0.3239 - val: 2.0400\n",
      "[026/030] train: 0.3124 - val: 2.0401\n",
      "[027/030] train: 0.2917 - val: 2.0932\n",
      "[028/030] train: 0.2793 - val: 2.0476\n",
      "[029/030] train: 0.2675 - val: 2.0815\n",
      "[030/030] train: 0.2589 - val: 2.0849\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 2000 \n",
    "n_epochs = 30\n",
    "# patience = 10\n",
    "# no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "# scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase in ('train', 'val'):\n",
    "        training = phase == 'train'\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "        batch_num = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    # scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # lr_history.extend(scheduler.get_lr())\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        # if phase == 'val':\n",
    "        #     if epoch_loss < best_loss:\n",
    "        #         print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "        #         best_loss = epoch_loss\n",
    "        #         best_weights = copy.deepcopy(net.state_dict())\n",
    "        #         no_improvements = 0\n",
    "        #     else:\n",
    "        #         no_improvements += 1\n",
    "                \n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    # if no_improvements >= patience:\n",
    "    #     print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 1.4421\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((np.array(predictions) - np.array(groud_truth))**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.75242329, 4.72158575, 3.67954445, ..., 4.11724758, 4.77862263,\n",
       "       3.99162102])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
