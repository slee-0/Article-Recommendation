{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282a6214",
   "metadata": {},
   "source": [
    "# Comparing Different Architecture Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a504d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consulted ChatGPT to write / debug this code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db9893",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147b3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.7536\n",
      "RMSE on test set: 1.7536\n"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV file into a DataFrame\n",
    "file_path =  '/Users/kaileyfitzgerald/Desktop/ML and Data Sci/final projrct/interactions_small.csv'  # Replace 'path_to_your_file.csv' with your file path\n",
    "ratings_data = pd.read_csv(file_path)\n",
    "\n",
    "# Create a Surprise Reader and Dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings_data[['user_id', 'book_id', 'rating']], reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use SVD algorithm (you can experiment with other algorithms as well)\n",
    "model = SVD()\n",
    "\n",
    "# Fit the model on the train set\n",
    "model.fit(trainset)\n",
    "\n",
    "# Test the model on the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "accuracy = rmse(predictions)\n",
    "print(f\"RMSE on test set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbdf38",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab725495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 3.7372, Validation Loss: 4.2585\n",
      "Epoch [2/10], Train Loss: 3.8863, Validation Loss: 4.0407\n",
      "Epoch [3/10], Train Loss: 4.0170, Validation Loss: 3.8229\n",
      "Epoch [4/10], Train Loss: 3.0864, Validation Loss: 3.6919\n",
      "Epoch [5/10], Train Loss: 3.3340, Validation Loss: 3.5586\n",
      "Epoch [6/10], Train Loss: 2.8904, Validation Loss: 3.5229\n",
      "Epoch [7/10], Train Loss: 3.2757, Validation Loss: 3.4073\n",
      "Epoch [8/10], Train Loss: 3.3052, Validation Loss: 3.4136\n",
      "Epoch [9/10], Train Loss: 2.1136, Validation Loss: 3.3677\n",
      "Epoch [10/10], Train Loss: 2.4426, Validation Loss: 3.3862\n",
      "RMSE on the test set: 1.8112\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have loaded your data into the following DataFrame\n",
    "# with columns: 'user_id', 'book_id', 'rating'\n",
    "ratings_data = pd.read_csv(file_path)\n",
    "\n",
    "# Creating mappings for user_id and book_id\n",
    "user_ids = ratings_data['user_id'].unique()\n",
    "book_ids = ratings_data['book_id'].unique()\n",
    "num_users = len(user_ids)\n",
    "num_books = len(book_ids)\n",
    "\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "book_to_idx = {book_id: idx for idx, book_id in enumerate(book_ids)}\n",
    "\n",
    "ratings_data['user_idx'] = ratings_data['user_id'].map(user_to_idx)\n",
    "ratings_data['book_idx'] = ratings_data['book_id'].map(book_to_idx)\n",
    "\n",
    "# Splitting data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(ratings_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data[['user_idx', 'book_idx', 'rating']].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Instantiate datasets and DataLoaders for train, validation, and test sets\n",
    "train_dataset = RatingDataset(train_data)\n",
    "val_dataset = RatingDataset(val_data)\n",
    "test_dataset = RatingDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define the RNN model\n",
    "class RNNRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_books, embedding_dim=32, hidden_dim=64):\n",
    "        super(RNNRecommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.book_embedding = nn.Embedding(num_books, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim * 2, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, user_idx, book_idx):\n",
    "        user_embed = self.user_embedding(user_idx)\n",
    "        book_embed = self.book_embedding(book_idx)\n",
    "        combined = torch.cat((user_embed, book_embed), dim=1)\n",
    "        output, _ = self.rnn(combined.unsqueeze(1))\n",
    "        output = self.fc(output.squeeze())\n",
    "        return output\n",
    "\n",
    "# Instantiate the model, define loss function and optimizer\n",
    "model = RNNRecommender(num_users, num_books)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        user_idx, book_idx, ratings = batch[:, 0], batch[:, 1], batch[:, 2].float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_idx, book_idx).squeeze()\n",
    "        loss = criterion(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for val_batch in val_loader:\n",
    "        val_user_idx, val_book_idx, val_ratings = val_batch[:, 0], val_batch[:, 1], val_batch[:, 2].float()\n",
    "        val_predictions = model(val_user_idx, val_book_idx).squeeze()\n",
    "        val_loss = criterion(val_predictions, val_ratings)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "predictions_list = []\n",
    "actual_ratings_list = []\n",
    "\n",
    "model.eval()\n",
    "for test_batch in test_loader:\n",
    "    test_user_idx, test_book_idx, test_ratings = test_batch[:, 0], test_batch[:, 1], test_batch[:, 2].float()\n",
    "    test_predictions = model(test_user_idx, test_book_idx).squeeze()\n",
    "\n",
    "    predictions_list.extend(test_predictions.tolist())\n",
    "    actual_ratings_list.extend(test_ratings.tolist())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "predictions_arr = np.array(predictions_list)\n",
    "actual_ratings_arr = np.array(actual_ratings_list)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((predictions_arr - actual_ratings_arr) ** 2))\n",
    "print(f\"RMSE on the test set: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b87d9a",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5cd178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 3.5365, Validation Loss: 3.9620\n",
      "Epoch [2/10], Train Loss: 3.9773, Validation Loss: 3.6732\n",
      "Epoch [3/10], Train Loss: 2.6881, Validation Loss: 3.5180\n",
      "Epoch [4/10], Train Loss: 2.1160, Validation Loss: 3.5500\n",
      "Epoch [5/10], Train Loss: 2.7185, Validation Loss: 3.6012\n",
      "Epoch [6/10], Train Loss: 1.9694, Validation Loss: 3.7021\n",
      "Epoch [7/10], Train Loss: 1.5401, Validation Loss: 3.8142\n",
      "Epoch [8/10], Train Loss: 1.2880, Validation Loss: 3.9416\n",
      "Epoch [9/10], Train Loss: 0.7933, Validation Loss: 4.0848\n",
      "Epoch [10/10], Train Loss: 0.6287, Validation Loss: 4.1794\n",
      "RMSE on the test set: 2.0178\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have loaded your data into the following DataFrame\n",
    "# with columns: 'user_id', 'book_id', 'rating'\n",
    "ratings_data = pd.read_csv(file_path)\n",
    "\n",
    "# Creating mappings for user_id and book_id\n",
    "user_ids = ratings_data['user_id'].unique()\n",
    "book_ids = ratings_data['book_id'].unique()\n",
    "num_users = len(user_ids)\n",
    "num_books = len(book_ids)\n",
    "\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "book_to_idx = {book_id: idx for idx, book_id in enumerate(book_ids)}\n",
    "\n",
    "ratings_data['user_idx'] = ratings_data['user_id'].map(user_to_idx)\n",
    "ratings_data['book_idx'] = ratings_data['book_id'].map(book_to_idx)\n",
    "\n",
    "# Splitting data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(ratings_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data[['user_idx', 'book_idx', 'rating']].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Instantiate datasets and DataLoaders for train, validation, and test sets\n",
    "train_dataset = RatingDataset(train_data)\n",
    "val_dataset = RatingDataset(val_data)\n",
    "test_dataset = RatingDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLPRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_books, hidden_dim=64):\n",
    "        super(MLPRecommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, hidden_dim)\n",
    "        self.book_embedding = nn.Embedding(num_books, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, user_idx, book_idx):\n",
    "        user_embed = self.user_embedding(user_idx)\n",
    "        book_embed = self.book_embedding(book_idx)\n",
    "        combined = torch.cat((user_embed, book_embed), dim=1)\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model, define loss function and optimizer\n",
    "model = MLPRecommender(num_users, num_books)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        user_idx, book_idx, ratings = batch[:, 0], batch[:, 1], batch[:, 2].float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_idx, book_idx).squeeze()\n",
    "        loss = criterion(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for val_batch in val_loader:\n",
    "        val_user_idx, val_book_idx, val_ratings = val_batch[:, 0], val_batch[:, 1], val_batch[:, 2].float()\n",
    "        val_predictions = model(val_user_idx, val_book_idx).squeeze()\n",
    "        val_loss = criterion(val_predictions, val_ratings)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "predictions_list = []\n",
    "actual_ratings_list = []\n",
    "\n",
    "model.eval()\n",
    "for test_batch in test_loader:\n",
    "    test_user_idx, test_book_idx, test_ratings = test_batch[:, 0], test_batch[:, 1], test_batch[:, 2].float()\n",
    "    test_predictions = model(test_user_idx, test_book_idx).squeeze()\n",
    "\n",
    "    predictions_list.extend(test_predictions.tolist())\n",
    "    actual_ratings_list.extend(test_ratings.tolist())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "predictions_arr = np.array(predictions_list)\n",
    "actual_ratings_arr = np.array(actual_ratings_list)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((predictions_arr - actual_ratings_arr) ** 2))\n",
    "print(f\"RMSE on the test set: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44daba67",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012a7fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 3.3112, Validation Loss: 4.2826\n",
      "Epoch [2/10], Train Loss: 2.7776, Validation Loss: 3.9669\n",
      "Epoch [3/10], Train Loss: 3.8131, Validation Loss: 3.7371\n",
      "Epoch [4/10], Train Loss: 3.5775, Validation Loss: 3.6085\n",
      "Epoch [5/10], Train Loss: 3.1538, Validation Loss: 3.4857\n",
      "Epoch [6/10], Train Loss: 2.9954, Validation Loss: 3.4582\n",
      "Epoch [7/10], Train Loss: 2.5588, Validation Loss: 3.4439\n",
      "Epoch [8/10], Train Loss: 1.9476, Validation Loss: 3.4658\n",
      "Epoch [9/10], Train Loss: 1.6697, Validation Loss: 3.4890\n",
      "Epoch [10/10], Train Loss: 1.7356, Validation Loss: 3.5240\n",
      "RMSE on the test set: 1.8509\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have loaded your data into the following DataFrame\n",
    "# with columns: 'user_id', 'book_id', 'rating'\n",
    "ratings_data = pd.read_csv(file_path)\n",
    "\n",
    "# Creating mappings for user_id and book_id\n",
    "user_ids = ratings_data['user_id'].unique()\n",
    "book_ids = ratings_data['book_id'].unique()\n",
    "num_users = len(user_ids)\n",
    "num_books = len(book_ids)\n",
    "\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "book_to_idx = {book_id: idx for idx, book_id in enumerate(book_ids)}\n",
    "\n",
    "ratings_data['user_idx'] = ratings_data['user_id'].map(user_to_idx)\n",
    "ratings_data['book_idx'] = ratings_data['book_id'].map(book_to_idx)\n",
    "\n",
    "# Splitting data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(ratings_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data[['user_idx', 'book_idx', 'rating']].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Instantiate datasets and DataLoaders for train, validation, and test sets\n",
    "train_dataset = RatingDataset(train_data)\n",
    "val_dataset = RatingDataset(val_data)\n",
    "test_dataset = RatingDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_books, embedding_dim=32, hidden_dim=64):\n",
    "        super(LSTMRecommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.book_embedding = nn.Embedding(num_books, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim * 2, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, user_idx, book_idx):\n",
    "        user_embed = self.user_embedding(user_idx)\n",
    "        book_embed = self.book_embedding(book_idx)\n",
    "        combined = torch.cat((user_embed, book_embed), dim=1)\n",
    "        output, _ = self.lstm(combined.unsqueeze(1))\n",
    "        output = self.fc(output.squeeze())\n",
    "        return output\n",
    "\n",
    "# Instantiate the model, define loss function and optimizer\n",
    "model = LSTMRecommender(num_users, num_books)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        user_idx, book_idx, ratings = batch[:, 0], batch[:, 1], batch[:, 2].float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_idx, book_idx).squeeze()\n",
    "        loss = criterion(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for val_batch in val_loader:\n",
    "        val_user_idx, val_book_idx, val_ratings = val_batch[:, 0], val_batch[:, 1], val_batch[:, 2].float()\n",
    "        val_predictions = model(val_user_idx, val_book_idx).squeeze()\n",
    "        val_loss = criterion(val_predictions, val_ratings)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "predictions_list = []\n",
    "actual_ratings_list = []\n",
    "\n",
    "model.eval()\n",
    "for test_batch in test_loader:\n",
    "    test_user_idx, test_book_idx, test_ratings = test_batch[:, 0], test_batch[:, 1], test_batch[:, 2].float()\n",
    "    test_predictions = model(test_user_idx, test_book_idx).squeeze()\n",
    "\n",
    "    predictions_list.extend(test_predictions.tolist())\n",
    "    actual_ratings_list.extend(test_ratings.tolist())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "predictions_arr = np.array(predictions_list)\n",
    "actual_ratings_arr = np.array(actual_ratings_list)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((predictions_arr - actual_ratings_arr) ** 2))\n",
    "print(f\"RMSE on the test set: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fedccdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 4.6175, Validation Loss: 4.3145\n",
      "Epoch [2/10], Train Loss: 3.5785, Validation Loss: 4.0870\n",
      "Epoch [3/10], Train Loss: 3.3141, Validation Loss: 3.9002\n",
      "Epoch [4/10], Train Loss: 3.4085, Validation Loss: 3.7357\n",
      "Epoch [5/10], Train Loss: 4.1850, Validation Loss: 3.5975\n",
      "Epoch [6/10], Train Loss: 3.1434, Validation Loss: 3.5376\n",
      "Epoch [7/10], Train Loss: 3.1276, Validation Loss: 3.4495\n",
      "Epoch [8/10], Train Loss: 2.5021, Validation Loss: 3.4983\n",
      "Epoch [9/10], Train Loss: 1.8358, Validation Loss: 3.4359\n",
      "Epoch [10/10], Train Loss: 2.2710, Validation Loss: 3.3932\n",
      "RMSE on the test set: 1.8302\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have loaded your data into the following DataFrame\n",
    "# with columns: 'user_id', 'book_id', 'rating'\n",
    "ratings_data = pd.read_csv(file_path)\n",
    "\n",
    "# Creating mappings for user_id and book_id\n",
    "user_ids = ratings_data['user_id'].unique()\n",
    "book_ids = ratings_data['book_id'].unique()\n",
    "num_users = len(user_ids)\n",
    "num_books = len(book_ids)\n",
    "\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "book_to_idx = {book_id: idx for idx, book_id in enumerate(book_ids)}\n",
    "\n",
    "ratings_data['user_idx'] = ratings_data['user_id'].map(user_to_idx)\n",
    "ratings_data['book_idx'] = ratings_data['book_id'].map(book_to_idx)\n",
    "\n",
    "# Splitting data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(ratings_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data[['user_idx', 'book_idx', 'rating']].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Instantiate datasets and DataLoaders for train, validation, and test sets\n",
    "train_dataset = RatingDataset(train_data)\n",
    "val_dataset = RatingDataset(val_data)\n",
    "test_dataset = RatingDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define the RNN model with embeddings\n",
    "class RNNWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_users, num_books, embedding_dim=32, hidden_dim=64):\n",
    "        super(RNNWithEmbeddings, self).__init__()\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        self.book_embeddings = nn.Embedding(num_books, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim * 2, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, user_idx, book_idx):\n",
    "        user_embed = self.user_embeddings(user_idx)\n",
    "        book_embed = self.book_embeddings(book_idx)\n",
    "        combined = torch.cat((user_embed, book_embed), dim=1)\n",
    "        output, _ = self.rnn(combined.unsqueeze(1))\n",
    "        output = self.fc(output.squeeze())\n",
    "        return output\n",
    "\n",
    "# Instantiate the model, define loss function and optimizer\n",
    "model = RNNWithEmbeddings(num_users, num_books)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        user_idx, book_idx, ratings = batch[:, 0], batch[:, 1], batch[:, 2].float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_idx, book_idx).squeeze()\n",
    "        loss = criterion(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for val_batch in val_loader:\n",
    "        val_user_idx, val_book_idx, val_ratings = val_batch[:, 0], val_batch[:, 1], val_batch[:, 2].float()\n",
    "        val_predictions = model(val_user_idx, val_book_idx).squeeze()\n",
    "        val_loss = criterion(val_predictions, val_ratings)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "predictions_list = []\n",
    "actual_ratings_list = []\n",
    "\n",
    "model.eval()\n",
    "for test_batch in test_loader:\n",
    "    test_user_idx, test_book_idx, test_ratings = test_batch[:, 0], test_batch[:, 1], test_batch[:, 2].float()\n",
    "    test_predictions = model(test_user_idx, test_book_idx).squeeze()\n",
    "\n",
    "    predictions_list.extend(test_predictions.tolist())\n",
    "    actual_ratings_list.extend(test_ratings.tolist())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "predictions_arr = np.array(predictions_list)\n",
    "actual_ratings_arr = np.array(actual_ratings_list)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((predictions_arr - actual_ratings_arr) ** 2))\n",
    "print(f\"RMSE on the test set: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe4bae",
   "metadata": {},
   "source": [
    "## MLP with Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26727c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 3.9311, Validation Loss: 4.1931\n",
      "Epoch [2/10], Train Loss: 4.2026, Validation Loss: 3.8984\n",
      "Epoch [3/10], Train Loss: 2.4567, Validation Loss: 3.7429\n",
      "Epoch [4/10], Train Loss: 2.8310, Validation Loss: 3.6405\n",
      "Epoch [5/10], Train Loss: 2.0208, Validation Loss: 3.6170\n",
      "Epoch [6/10], Train Loss: 2.5099, Validation Loss: 3.6965\n",
      "Epoch [7/10], Train Loss: 2.3580, Validation Loss: 3.6498\n",
      "Epoch [8/10], Train Loss: 2.2615, Validation Loss: 3.8220\n",
      "Epoch [9/10], Train Loss: 1.9964, Validation Loss: 3.9691\n",
      "Epoch [10/10], Train Loss: 1.9048, Validation Loss: 3.8894\n",
      "RMSE on the test set: 1.9353\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have loaded your data into the following DataFrame\n",
    "# with columns: 'user_id', 'book_id', 'rating'\n",
    "ratings_data = pd.read_csv(file_path)\n",
    "\n",
    "# Creating mappings for user_id and book_id\n",
    "user_ids = ratings_data['user_id'].unique()\n",
    "book_ids = ratings_data['book_id'].unique()\n",
    "num_users = len(user_ids)\n",
    "num_books = len(book_ids)\n",
    "\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "book_to_idx = {book_id: idx for idx, book_id in enumerate(book_ids)}\n",
    "\n",
    "ratings_data['user_idx'] = ratings_data['user_id'].map(user_to_idx)\n",
    "ratings_data['book_idx'] = ratings_data['book_id'].map(book_to_idx)\n",
    "\n",
    "# Splitting data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(ratings_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data[['user_idx', 'book_idx', 'rating']].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Instantiate datasets and DataLoaders for train, validation, and test sets\n",
    "train_dataset = RatingDataset(train_data)\n",
    "val_dataset = RatingDataset(val_data)\n",
    "test_dataset = RatingDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define the MLP model with embeddings\n",
    "class MLPWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_users, num_books, embedding_dim=32, hidden_dim=64):\n",
    "        super(MLPWithEmbeddings, self).__init__()\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        self.book_embeddings = nn.Embedding(num_books, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, user_idx, book_idx):\n",
    "        user_embed = self.user_embeddings(user_idx)\n",
    "        book_embed = self.book_embeddings(book_idx)\n",
    "        combined = torch.cat((user_embed, book_embed), dim=1)\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model, define loss function and optimizer\n",
    "model = MLPWithEmbeddings(num_users, num_books)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        user_idx, book_idx, ratings = batch[:, 0], batch[:, 1], batch[:, 2].float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_idx, book_idx).squeeze()\n",
    "        loss = criterion(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for val_batch in val_loader:\n",
    "        val_user_idx, val_book_idx, val_ratings = val_batch[:, 0], val_batch[:, 1], val_batch[:, 2].float()\n",
    "        val_predictions = model(val_user_idx, val_book_idx).squeeze()\n",
    "        val_loss = criterion(val_predictions, val_ratings)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "predictions_list = []\n",
    "actual_ratings_list = []\n",
    "\n",
    "model.eval()\n",
    "for test_batch in test_loader:\n",
    "    test_user_idx, test_book_idx, test_ratings = test_batch[:, 0], test_batch[:, 1], test_batch[:, 2].float()\n",
    "    test_predictions = model(test_user_idx, test_book_idx).squeeze()\n",
    "\n",
    "    predictions_list.extend(test_predictions.tolist())\n",
    "    actual_ratings_list.extend(test_ratings.tolist())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "predictions_arr = np.array(predictions_list)\n",
    "actual_ratings_arr = np.array(actual_ratings_list)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((predictions_arr - actual_ratings_arr) ** 2))\n",
    "print(f\"RMSE on the test set: {rmse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
